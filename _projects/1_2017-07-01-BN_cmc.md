---
title: "Why Batch Normalization Works so Well?"
collection: projects
type: "Final project"
permalink: /projects/1_2017-07-01-BN_cmc
date: 2017-07-01
venue: 'Machine Learning having it deep and structured, NTU, Spring 2017.'
paperurl: 'http://nothinghard.github.io/files/BN_cmc.pdf'
display: '../images/crime_all.png'
---

Batch normalization (BN) is regarded as the necessary component in many well-known neural network architectures to speed up network training phrase and also to improve performance.
This project aims at validating the effectiveness of BN claimed by the authors, and we make several interesting findings:
- BN does **regularize the growth of weights** by checking the absolute values of weights
- BN does **tackle gradient vanishing** problem by monitoring the magnitude of gradients
- BN does **benefit the gradient flows** through the network by shifting the distribution of **singular values of layer's Jacobian** closer to one
- >> The distribution of singular values of layer's Jacobian reveals the isometry of error propagation through the network
This work not only shows the effectiveness of BN by comprehensive experiments but also investigates the underlying reasons to understand, in the theoretical point of view, that why batch normalization works so well.



