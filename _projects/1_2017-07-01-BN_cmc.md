---
title: "Why Batch Normalization Works so Well?"
collection: projects
type: "Final project"
permalink: /projects/1_2017-07-01-BN_cmc
date: 2017-07-01
venue: 'Machine Learning having it deep and structured, NTU, Spring 2017'
paperurl: 'http://nothinghard.github.io/files/BN_cmc.pdf'
display: '../images/bn_singular.png'
---

**Batch normalization** (BN) is regarded as the necessary component in many well-known neural network architectures to speed up network training phrase and also to improve performance. In this work, we investigate the effectiveness of BN by comprehensive experiments and also reveal the underlying reasons, in the theoretical point of view, that **why batch normalization works so well**. We have demonstrate the following effects:
- BN does **regularize** the growth of weights (verifed by checking weight distribution)
- BN does **tackle gradient vanishing** problem (verified by monitoring gradient's magnitude)
- BN does **improve gradient flows** of network (verified by examining the isometry of backward propagation)
- We compare the distribution of **singular values of layer's Jacobian** to verify the claim that BN leads to better isometry when error propagates through networks.
- Github repository >> [https://github.com/NothingHard/MLDS_final_2017](https://github.com/NothingHard/MLDS_final_2017)

## Slideshare
<iframe src="//www.slideshare.net/slideshow/embed_code/key/GDTNdQPxwOQ02I" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ssuser950871/why-batch-normalization-works-so-well" title="Why Batch Normalization Works so Well" target="_blank">Why Batch Normalization Works so Well</a> </strong> from <strong><a href="https://www.slideshare.net/ssuser950871" target="_blank">Chun-Ming Chang</a></strong> </div>

## Why Needs Batch Normalization?
Batch normalization is to accelerate network training by reducing internal covariate shift. **Internal covariate shift** is the change in the distribution of network activations due to the change in network parameters during training. By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.

The full whitening of each layerâ€™s inputs is costly and not everywhere differentiable, so, in batch normalization, there are two necessary simplifications:
1. instead of whitening the features in layer inputs and outputs jointly, **independently normalize** each scalar feature
2. use **mini-batches statistics** rather than global statistics in stochastic gradient training

## Experimental Study

We conduct experiments on MNIST using a simple fully connected neural network (3 layers), and 

## Default setting of our model

| Parameter | Selection |
|-----------|-----------|
|  |  |

## Validation of the claims made by the authors
### 1. BN regularizes the model
...
### 2. BN solves the gradient vanishing problem
...
<img src="../images/bn_exp456.png" style="width:100%;">

### 3. BN benefits gradient flows through network
- The distribution of singular values of layer's Jacobian

<img src="../images/l1_relu.gif" style="width:48%;" loop="-1">
<img src="../images/l2_sigmoid.gif" style="width:48%;" loop="-1">

## Which the following factors affect the effect of BN most?

| Experiment | Alternatives |
|------------|--------------|
| 1. Activation function |  lrelu, relu sigmoid, softplus, tanh |
| 2. Optimizer  | Adam, SGD |
| 3. Batch size | 4, 16, 64, 256 |

<img src="../images/bn_exp123.png" style="width:100%;">


## 

## Summary II

